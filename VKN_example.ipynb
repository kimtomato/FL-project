{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VKN_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMANR4K7lUsOXkK4BrpGRgr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimtomato/FL_KCN-project/blob/master/VKN_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO_0qIGg3GeQ",
        "outputId": "1fb23253-7f7f-414f-b9dd-8b58b5940e19"
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.6.0\n",
            "Uninstalling tensorflow-2.6.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.6.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWFwgjDX3a7W",
        "outputId": "34a5371c-648c-4ae2-cb70-4f5559880c17"
      },
      "source": [
        "!pip uninstall keras"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 2.6.0\n",
            "Uninstalling keras-2.6.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/keras-2.6.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled keras-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39SEnjN438JZ",
        "outputId": "eea5f80e-2650-41f2-a8a6-112a85b97829"
      },
      "source": [
        "!pip install tensorflow==2.5.0\n",
        "!pip install tensorflow_federated==0.19.0\n",
        "!pip install keras==2.5.0rc0\n",
        "!pip install tensorflow_privacy==0.7.3"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.34.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.5.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.12.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.6.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.17.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.5.0) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.5.0)\n",
            "Requirement already satisfied: tensorflow_federated==0.19.0 in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Requirement already satisfied: jaxlib~=0.1.55 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (0.1.71+cuda111)\n",
            "Requirement already satisfied: tensorflow-privacy~=0.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (0.5.2)\n",
            "Requirement already satisfied: jax~=0.2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (0.2.20)\n",
            "Requirement already satisfied: attrs~=19.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (19.3.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization~=0.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (0.5.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (0.1.6)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (1.34.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (0.12.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (1.19.5)\n",
            "Requirement already satisfied: retrying~=1.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (1.3.3)\n",
            "Requirement already satisfied: semantic-version~=2.8.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (2.8.5)\n",
            "Requirement already satisfied: portpicker~=1.3.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (1.3.9)\n",
            "Requirement already satisfied: tensorflow~=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (2.5.0)\n",
            "Requirement already satisfied: cachetools~=3.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (3.1.1)\n",
            "Requirement already satisfied: tqdm~=4.28.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_federated==0.19.0) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py~=0.10->tensorflow_federated==0.19.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax~=0.2.8->tensorflow_federated==0.19.0) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax~=0.2.8->tensorflow_federated==0.19.0) (1.4.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib~=0.1.55->tensorflow_federated==0.19.0) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (0.37.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (3.7.4.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.1.2)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (3.17.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (2.6.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.5.0->tensorflow_federated==0.19.0) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (3.3.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (3.1.1)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from tensorflow-privacy~=0.5.0->tensorflow_federated==0.19.0) (1.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow~=2.5.0->tensorflow_federated==0.19.0) (3.5.0)\n",
            "Requirement already satisfied: keras==2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (2.5.0rc0)\n",
            "Collecting tensorflow_privacy==0.7.3\n",
            "  Downloading tensorflow_privacy-0.7.3-py3-none-any.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting attrs>=21.2.0\n",
            "  Downloading attrs-21.2.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow-datasets>=4.4.0\n",
            "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-estimator>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy==0.7.3) (2.5.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy==0.7.3) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-probability>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy==0.7.3) (0.13.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy==0.7.3) (1.2.1)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from tensorflow_privacy==0.7.3) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from dm-tree~=0.1.1->tensorflow_privacy==0.7.3) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.17->tensorflow_privacy==0.7.3) (1.19.5)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (0.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (0.12.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (5.2.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (3.7.4.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (3.17.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (4.28.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (0.3.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.13.0->tensorflow_privacy==0.7.3) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.13.0->tensorflow_privacy==0.7.3) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.13.0->tensorflow_privacy==0.7.3) (0.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (3.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets>=4.4.0->tensorflow_privacy==0.7.3) (1.53.0)\n",
            "Installing collected packages: attrs, tensorflow-datasets, tensorflow-privacy\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 19.3.0\n",
            "    Uninstalling attrs-19.3.0:\n",
            "      Successfully uninstalled attrs-19.3.0\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "  Attempting uninstall: tensorflow-privacy\n",
            "    Found existing installation: tensorflow-privacy 0.5.2\n",
            "    Uninstalling tensorflow-privacy-0.5.2:\n",
            "      Successfully uninstalled tensorflow-privacy-0.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-federated 0.19.0 requires attrs~=19.3.0, but you have attrs 21.2.0 which is incompatible.\n",
            "tensorflow-federated 0.19.0 requires tensorflow-privacy~=0.5.0, but you have tensorflow-privacy 0.7.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed attrs-21.2.0 tensorflow-datasets-4.4.0 tensorflow-privacy-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G78Y7EPaKaQ8"
      },
      "source": [
        "!pip install --quiet --upgrade nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkKzyId-onWw",
        "outputId": "94044e26-0788-43dc-b4ca-7296fd63f98d"
      },
      "source": [
        "#fed.py\n",
        "import collections\n",
        "import random\n",
        "\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 20\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "    def batch_format_fn(element):\n",
        "        \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "        return collections.OrderedDict(\n",
        "            x=tf.reshape(element['pixels'], [-1, 784]),\n",
        "            y=tf.reshape(element['label'], [-1, 1]))\n",
        "\n",
        "    return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
        "        BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "\n",
        "def make_federated_data(client_data, client_ids):\n",
        "    return [\n",
        "        preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "        for x in client_ids\n",
        "    ]\n",
        "\n",
        "\n",
        "def create_keras_model():\n",
        "    return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=(784,)),\n",
        "        tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
        "        tf.keras.layers.Softmax(),\n",
        "    ])\n",
        "\n",
        "\n",
        "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()\n",
        "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
        "    emnist_train.client_ids[0])\n",
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "\n",
        "\n",
        "def model_fn():\n",
        "    # We _must_ create a new model here, and _not_ capture it from an external\n",
        "    # scope. TFF will call this within different graph contexts.\n",
        "    keras_model = create_keras_model()\n",
        "    return tff.learning.from_keras_model(\n",
        "        keras_model,\n",
        "        input_spec=preprocessed_example_dataset.element_spec,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "\n",
        "evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
        "\n",
        "\n",
        "class FLData:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.emnist_train, self.emnist_test = tff.simulation.datasets.emnist.load_data()\n",
        "\n",
        "    def get_nb_samples(self):\n",
        "        return len(self.emnist_train.client_ids)\n",
        "\n",
        "    def get_data_samples_id(self, nb_samples):\n",
        "        return random.choices(self.emnist_train.client_ids, k=nb_samples)\n",
        "\n",
        "    def get_federated_data(self, sample_ids):\n",
        "        return make_federated_data(self.emnist_train, sample_ids)\n",
        "\n",
        "    def get_federated_test_data(self, sample_ids):\n",
        "        return make_federated_data(self.emnist_test, sample_ids)\n",
        "\n",
        "    def get_all_samples_id(self):\n",
        "        return self.emnist_train.client_ids\n",
        "\n",
        "    def get_all_test_samples_id(self):\n",
        "        return self.emnist_test.client_ids\n",
        "\n",
        "\n",
        "class FLDataSelector:\n",
        "\n",
        "    def __init__(self, worlddim, mode, fl_data, cut):\n",
        "\n",
        "        self.div = cut\n",
        "        if mode == \"localized\":\n",
        "            print('SELECTOR: {}x{} cells'.format(cut, cut))\n",
        "\n",
        "        self.dim = float(worlddim)\n",
        "        self.fl_data = fl_data\n",
        "        if mode == \"random\" or mode == \"localized\":\n",
        "            self.mode = mode\n",
        "        else:\n",
        "            self.mode = \"random\"\n",
        "\n",
        "        self.slices = None\n",
        "        if mode == \"localized\":\n",
        "            # Generate location-based data.\n",
        "            size = self.div\n",
        "            self.slices = []\n",
        "            for i in range(size):\n",
        "                self.slices.append(self.fl_data.get_data_samples_id(size))\n",
        "\n",
        "    def get_dataslice(self, x, y):\n",
        "\n",
        "        if self.mode == \"localized\":\n",
        "\n",
        "            x_ = min(max(x, 0), self.dim - 0.01)\n",
        "            y_ = min(max(y, 0), self.dim - 0.01)\n",
        "\n",
        "            #print (x_,y_)\n",
        "\n",
        "            i = int(x_ / self.dim * self.div)\n",
        "            j = int(y_ / self.dim * self.div)\n",
        "\n",
        "            #print (i,j)\n",
        "            #print (self.slices[i][j])\n",
        "            return self.slices[i][j]\n",
        "\n",
        "        else:\n",
        "            return self.fl_data.get_data_samples_id(1)[0]\n",
        "\n",
        "\n",
        "class FLTraining:\n",
        "    @staticmethod\n",
        "    def set_seed(seed):\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "    def __init__(self, validation_set):\n",
        "\n",
        "        self.iterative_process = tff.learning.build_federated_averaging_process(\n",
        "            model_fn, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(\n",
        "                learning_rate=0.02), server_optimizer_fn=lambda: tf.keras.optimizers.SGD(\n",
        "                learning_rate=1.0))\n",
        "\n",
        "        self.state = self.iterative_process.initialize()\n",
        "        self.validation_set = validation_set\n",
        "        self.round_id = 0\n",
        "\n",
        "        self.current_evaluation = None\n",
        "\n",
        "    def training_round(self, federated_data):\n",
        "\n",
        "        if len(federated_data) == 0:\n",
        "            print('round {:2d}, skip.'.format(self.round_id))\n",
        "\n",
        "        else:\n",
        "            self.state, _ = self.iterative_process.next(\n",
        "                self.state, federated_data)\n",
        "            self.current_evaluation = evaluation(\n",
        "                self.state.model, self.validation_set)\n",
        "            print('round {:2d}, eval_metrics={}'.format(\n",
        "                self.round_id, self.current_evaluation))\n",
        "\n",
        "        print(\"size:{}\".format(sys.getsizeof(self.state.model)))\n",
        "\n",
        "        self.round_id += 1\n",
        "        return self.current_evaluation\n",
        "\n",
        "'''\n",
        "NUM_CLIENTS = 10\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
        "\n",
        "\n",
        "logdir = \"logs/training/\"\n",
        "summary_writer = tf.summary.create_file_writer(logdir)\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "\n",
        "sample_clients = emnist_train.client_ids[0:NUM_CLIENTS]\n",
        "\n",
        "federated_train_data = make_federated_data(emnist_train, sample_clients)\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "if len(federated_train_data) > 0:\n",
        "    print('First dataset: {d}'.format(d=federated_train_data[0]))\n",
        "\n",
        "\n",
        "evaluation = tff.learning.build_federated_evaluation(model_fn)\n",
        "\n",
        "x = []\n",
        "y1 = []\n",
        "Y1 = []\n",
        "y2 = []\n",
        "Y2 = []\n",
        "\n",
        "\n",
        "NUM_ROUNDS = 80\n",
        "#with summary_writer.as_default():\n",
        "for round_num in range(1, NUM_ROUNDS):\n",
        "\n",
        "    clients_set = random.choices(emnist_train.client_ids, k=10)\n",
        "    print(\"round {} clients:{}\".format(round_num, clients_set))\n",
        "    federated_train_data = make_federated_data(emnist_train, clients_set)\n",
        "    federated_test_data = make_federated_data(emnist_test, emnist_train.client_ids[0:50])\n",
        "\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "\n",
        "    eval_metrics = evaluation(state.model, federated_test_data)\n",
        "    print('round {:2d}, eval_metrics={}'.format(round_num, eval_metrics))\n",
        "\n",
        "    x.append(round_num)\n",
        "    y1.append(eval_metrics['loss'])\n",
        "    Y1.append(eval_metrics['sparse_categorical_accuracy'])\n",
        "\n",
        "    #for name, value in dict(eval_metrics).items():\n",
        "    #    tf.summary.scalar(name, value, step=round_num)\n",
        "\n",
        "    #print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "\n",
        "\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "for round_num in range(1, NUM_ROUNDS):\n",
        "\n",
        "    clients_set = random.choices(emnist_train.client_ids, k=random.randint(1,10))\n",
        "    print(\"round {} clients:{}\".format(round_num, clients_set))\n",
        "    federated_train_data = make_federated_data(emnist_train, clients_set)\n",
        "    federated_test_data = make_federated_data(emnist_test, emnist_train.client_ids[0:50])\n",
        "\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "\n",
        "    eval_metrics = evaluation(state.model, federated_test_data)\n",
        "    print('round {:2d}, eval_metrics={}'.format(round_num, eval_metrics))\n",
        "\n",
        "    y2.append(eval_metrics['loss'])\n",
        "    Y2.append(eval_metrics['sparse_categorical_accuracy'])\n",
        "\n",
        "    #for name, value in dict(eval_metrics).items():\n",
        "    #    tf.summary.scalar(name, value, step=round_num)\n",
        "\n",
        "    #print('round {:2d}, metrics={}'.format(round_num, metrics))\n",
        "\n",
        "\n",
        "\n",
        "plt.title('Loss')\n",
        "plt.xlabel('round number')\n",
        "plt.ylabel('loss')\n",
        "plt.plot(x,y1,label='10 clients')\n",
        "plt.plot(x,y2, label='2 clients')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.title('Acc')\n",
        "plt.xlabel('round number')\n",
        "plt.ylabel('Acc')\n",
        "plt.plot(x,Y1,label='10 clients')\n",
        "plt.plot(x,Y2, label='2 clients')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading emnist_all.sqlite.lzma:  91%|█████████ | 155189248/170507172 [01:11<00:08, 1858963.83it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "8eIvL1QLpjVJ",
        "outputId": "126290c2-d34c-4672-a398-06b409737a74"
      },
      "source": [
        "#Mobility.py\n",
        "import numpy as np\n",
        "\n",
        "PI = 3.14159\n",
        "\n",
        "\n",
        "class World:\n",
        "    \"\"\"A simple class holding information about the size of the area containing all the\n",
        "    vehicles and helper methods to compute points and distances.\"\"\"\n",
        "\n",
        "    def __init__(self, dimension):\n",
        "        self.dimension = dimension\n",
        "\n",
        "    def random_point(self):\n",
        "        return np.random.uniform(high=self.dimension, size=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def random_point_within_circle(center, radius):\n",
        "        r = np.random.uniform() * radius\n",
        "        angle = np.random.uniform() * 2 * PI\n",
        "\n",
        "        return center + r * np.array((np.cos(angle), np.sin(angle)))\n",
        "\n",
        "    @staticmethod\n",
        "    def distance(a, b):\n",
        "        return np.linalg.norm(a - b)\n",
        "\n",
        "    @staticmethod\n",
        "    def intermediate_point(a, b, distance):\n",
        "        (dy, dx) = (b[1] - a[1], b[0] - a[0])\n",
        "        angle = np.math.atan2(dy, dx)\n",
        "\n",
        "        return a + distance * np.array((np.cos(angle), np.sin(angle)))\n",
        "\n",
        "\n",
        "class MovingNode:\n",
        "    \"\"\" A class representing an abstract moving node. The vehicle is able to move within the bounds\n",
        "    described by a World instance, following a (steady-state) Random Waypoint mobility model.\"\"\"\n",
        "\n",
        "    def __init__(self, world, node_id):\n",
        "        self.n_id = node_id\n",
        "        self.position = None\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (self.n_id == other.n_id)\n",
        "\n",
        "    def move(self, timestep):\n",
        "        pass\n",
        "\n",
        "\n",
        "class RWPNode(MovingNode):\n",
        "    def __init__(self, world, node_id):\n",
        "        self.n_id = node_id\n",
        "        self.position = None\n",
        "        self.world = world\n",
        "\n",
        "        self.pause_probability = 0.5\n",
        "        self.pause_max = 300\n",
        "\n",
        "        self.min_speed = 5\n",
        "        self.max_speed = 20\n",
        "\n",
        "        self._init_initial_path()\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (self.n_id == other.n_id)\n",
        "\n",
        "    def _init_initial_path(self):\n",
        "        # Will the node begin in a paused state ?\n",
        "        u_pause = np.random.uniform()\n",
        "        if u_pause < self.pause_probability:\n",
        "\n",
        "            u = np.random.uniform()\n",
        "            self.current_pause_time = self.pause_max * (1 - np.sqrt(1 - u))\n",
        "            self.position = np.random.uniform(size=2) * self.world.dimension\n",
        "            self._destination = self.position  # Will be recomputed at the end of the pause\n",
        "            self._speed = 0\n",
        "\n",
        "            #print ('node id {}: starting in a pause of {}s at pos {}'.format(self.n_id, self.current_pause_time, self.position))\n",
        "\n",
        "        else:  # The node begins in movement\n",
        "\n",
        "            self.current_pause_time = 0\n",
        "\n",
        "            # Initial speed (steady-state)\n",
        "            u = np.random.uniform()\n",
        "            self._speed = (self.max_speed**u) / (self.min_speed**(u - 1))\n",
        "\n",
        "            # Initial path (steady-state)\n",
        "            path_chosen = False\n",
        "            while not path_chosen:\n",
        "                (x1, y1) = np.random.uniform(size=2)\n",
        "                (x2, y2) = np.random.uniform(size=2)\n",
        "\n",
        "                r = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / np.sqrt(2)\n",
        "                u1 = np.random.uniform()\n",
        "\n",
        "                if u1 < r:\n",
        "                    u2 = np.random.uniform()\n",
        "\n",
        "                    self.position = np.array(\n",
        "                        (u2 * x1 + (1 - u2) * x2, u2 * y1 + (1 - u2) * y2)) * self.world.dimension\n",
        "                    self._destination = np.array(\n",
        "                        (x2, y2)) * self.world.dimension\n",
        "                    path_chosen = True\n",
        "\n",
        "            #print (\"node id {}: starting in move: pos:{}, dest:{}, speed:{}\".format(self.n_id, self.position, self._destination, self._speed))\n",
        "\n",
        "    def _compute_new_pause(self):\n",
        "\n",
        "        #print (\"node id {}: computing new potential pause ?\".format(self.n_id))\n",
        "        u_pause = np.random.uniform()\n",
        "\n",
        "        if u_pause < self.pause_probability:\n",
        "            u_pausetime = np.random.uniform()\n",
        "            self.current_pause_time = u_pausetime * self.pause_max\n",
        "            #print (\"nid {}: there will be a pause of {}s\".format(self.n_id, self.current_pause_time))\n",
        "\n",
        "        else:  # No pause: compute new move\n",
        "            self.current_pause_time = 0\n",
        "            #print (\"nid {}: there will be no pause\".format(self.n_id))\n",
        "            self._compute_new_path()\n",
        "\n",
        "    def _compute_new_path(self):\n",
        "\n",
        "        # Path\n",
        "        self.position = self._destination\n",
        "        self._destination = np.random.uniform(size=2) * self.world.dimension\n",
        "\n",
        "        # Speed\n",
        "        u = np.random.uniform()\n",
        "        self._speed = self.min_speed + u * (self.max_speed - self.min_speed)\n",
        "\n",
        "        #print (\"nid {}: computed new path: current pos:{}, dest:{}, speed:{}\".format(self.n_id, self.position, self._destination, self._speed))\n",
        "\n",
        "    def move(self, timestep):\n",
        "\n",
        "        # State: pause ?\n",
        "        if self.current_pause_time > 0:\n",
        "            self.current_pause_time = np.maximum(\n",
        "                0, self.current_pause_time - timestep)\n",
        "            #print (\"nid {}: pausing at pos {}... remaining pause after this step:{}\".format(self.n_id, self.position, self.current_pause_time))\n",
        "\n",
        "            if self.current_pause_time == 0:  # Pause finished\n",
        "                self._compute_new_path()\n",
        "            return\n",
        "\n",
        "        # State: driving\n",
        "        distance_to_dest = World.distance(self.position, self._destination)\n",
        "        driven_distance = self._speed * float(timestep)\n",
        "\n",
        "        #print (\"driven distance:{}, dist to dest:{}\".format(driven_distance, distance_to_dest))\n",
        "\n",
        "        if driven_distance >= distance_to_dest:\n",
        "            # print(\"recomputing\")\n",
        "            self.position = self._destination\n",
        "            #print (\"nid {} arrived at destination {}={}\".format(self.n_id, self.position, self._destination))\n",
        "\n",
        "            self._compute_new_pause()\n",
        "\n",
        "        else:  # Continue the drive to waypoint\n",
        "            self.position = World.intermediate_point(\n",
        "                self.position, self._destination, driven_distance)\n",
        "            #print (\"nid {}: driving: pos:{}, dest:{}, new dist to dest:{}\".format(self.n_id, self.position, self._destination, World.distance(self.position, self._destination)))\n",
        "\n",
        "    # Helper function: useful for eventual RPGM Following nodes.\n",
        "\n",
        "    def get_reference_point_area(self):\n",
        "        return {'pos': self.position, 'radius': 25}\n",
        "\n",
        "    def get_max_speed(self):\n",
        "        return self.max_speed\n",
        "\n",
        "    def is_pausing(self):\n",
        "        return (self.current_pause_time > 0)\n",
        "\n",
        "    def get_angle(self):\n",
        "        a = self.position\n",
        "        b = self._destination\n",
        "\n",
        "        (dy, dx) = (b[1] - a[1], b[0] - a[0])\n",
        "        return np.math.atan2(dy, dx)\n",
        "\n",
        "\n",
        "class RPGMFollowingNode(MovingNode):\n",
        "    ''' Node that follows a group leader (the group leader follows steady-state RWP) '''\n",
        "\n",
        "    def __init__(self, leader, node_id):\n",
        "        self.n_id = node_id\n",
        "        self.leader = None\n",
        "\n",
        "        self.reference_point = None\n",
        "        self.ref_dist = None\n",
        "        self.set_group_leader(leader)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (self.n_id == other.n_id)\n",
        "\n",
        "    def set_group_leader(self, leader):\n",
        "        self.leader = leader\n",
        "        ref = self.leader.get_reference_point_area()\n",
        "\n",
        "        # Set initial position\n",
        "        self.reference_point = World.random_point_within_circle(ref['pos'], 25)\n",
        "        self.ref_dist = (\n",
        "            self.reference_point[0] -\n",
        "            self.leader.position[0],\n",
        "            self.reference_point[1] -\n",
        "            self.leader.position[1])\n",
        "\n",
        "        self.position = World.random_point_within_circle(\n",
        "            self.reference_point, 25)\n",
        "\n",
        "    def move(self, timestep):\n",
        "\n",
        "        # Follow the leader\n",
        "        if self.leader.is_pausing():\n",
        "            return\n",
        "\n",
        "        # Compute new position based on speed and angle\n",
        "        self.reference_point = np.array(\n",
        "            [self.leader.position[0] + self.ref_dist[0], self.leader.position[1] + self.ref_dist[1]])\n",
        "        self.position = World.random_point_within_circle(\n",
        "            self.reference_point, 25)\n",
        "\n",
        "        # self.update_speed_angle()\n",
        "        #self.position += timestep*self.speed*np.array((np.cos(self.angle), np.sin(self.angle)))\n",
        "\n",
        "        #ref = self.leader.get_reference_point()\n",
        "        #self.position = World.random_point_within_circle(self.leader.position, 50)\n",
        "\n",
        "        #print ('v{}, speed:{}, angle:{}'.format(self.n_id, self.speed, self.angle))\n",
        "\n",
        "\n",
        "# vehicles_count must be divisible by groups_count\n",
        "def generate_rpgm_mobility(groups_list, world):\n",
        "\n",
        "    nodes = []\n",
        "\n",
        "    i = 0\n",
        "    for group_size in groups_list:\n",
        "\n",
        "        leader = RWPNode(world, i)\n",
        "        nodes.append(leader)\n",
        "        #print(\"leader {}\".format(i))\n",
        "\n",
        "        for _ in range(group_size - 1):\n",
        "            i += 1\n",
        "            follower = RPGMFollowingNode(leader, i)\n",
        "            nodes.append(follower)\n",
        "            #print(\"follower {}\".format(i))\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return nodes\n",
        "\n",
        "\n",
        "'''\n",
        "world = World(1000)\n",
        "nodes = generate_rpgm_mobility(50, 5, world)\n",
        "\n",
        "\n",
        "timestep = 2\n",
        "for i in range(0,20):\n",
        "    for n in nodes:\n",
        "        n.move(timestep)\n",
        "\n",
        "    pt_leaders = []\n",
        "    pt_followers = []\n",
        "    for n in nodes:\n",
        "        if isinstance(n, RPGMFollowingNode):\n",
        "            pt_followers.append(n.position)\n",
        "        else:\n",
        "            pt_leaders.append(n.position)\n",
        "\n",
        "    pt_leaders = np.array(pt_leaders)\n",
        "    pt_followers = np.array(pt_followers)\n",
        "\n",
        "    plt.scatter(pt_followers[:, 0], pt_followers[:, 1], color='blue')\n",
        "    plt.scatter(pt_leaders[:, 0], pt_leaders[:, 1], color='orange', marker='x')\n",
        "    plt.xlim((0,world.dimension))\n",
        "    plt.ylim((0,world.dimension))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "world = World(1000)\n",
        "leader = RWPNode(world, 1)\n",
        "\n",
        "v1 = RPGMFollowingNode(leader, 2)\n",
        "v2 = RPGMFollowingNode(leader, 3)\n",
        "v3 = RPGMFollowingNode(leader, 4)\n",
        "\n",
        "\n",
        "timestep = 2\n",
        "for i in range(0,100000):\n",
        "    leader.move(timestep)\n",
        "    v1.move(timestep)\n",
        "    v2.move(timestep)\n",
        "    v3.move(timestep)\n",
        "\n",
        "point_leader = np.array([leader.position])\n",
        "vehs = np.array([v1.position, v2.position, v3.position])\n",
        "\n",
        "plt.scatter(vehs[:, 0], vehs[:, 1], color='blue')\n",
        "plt.scatter(point_leader[:, 0], point_leader[:, 1], color='orange', marker='x')\n",
        "plt.xlim((0,world.dimension))\n",
        "plt.ylim((0,world.dimension))\n",
        "plt.show()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nworld = World(1000)\\nnodes = generate_rpgm_mobility(50, 5, world)\\n\\n\\ntimestep = 2\\nfor i in range(0,20):\\n    for n in nodes:\\n        n.move(timestep)\\n\\n    pt_leaders = []\\n    pt_followers = []\\n    for n in nodes:\\n        if isinstance(n, RPGMFollowingNode):\\n            pt_followers.append(n.position)\\n        else:\\n            pt_leaders.append(n.position)\\n\\n    pt_leaders = np.array(pt_leaders)\\n    pt_followers = np.array(pt_followers)\\n\\n    plt.scatter(pt_followers[:, 0], pt_followers[:, 1], color='blue')\\n    plt.scatter(pt_leaders[:, 0], pt_leaders[:, 1], color='orange', marker='x')\\n    plt.xlim((0,world.dimension))\\n    plt.ylim((0,world.dimension))\\n    plt.show()\\n\\n\\nworld = World(1000)\\nleader = RWPNode(world, 1)\\n\\nv1 = RPGMFollowingNode(leader, 2)\\nv2 = RPGMFollowingNode(leader, 3)\\nv3 = RPGMFollowingNode(leader, 4)\\n\\n\\ntimestep = 2\\nfor i in range(0,100000):\\n    leader.move(timestep)\\n    v1.move(timestep)\\n    v2.move(timestep)\\n    v3.move(timestep)\\n\\npoint_leader = np.array([leader.position])\\nvehs = np.array([v1.position, v2.position, v3.position])\\n\\nplt.scatter(vehs[:, 0], vehs[:, 1], color='blue')\\nplt.scatter(point_leader[:, 0], point_leader[:, 1], color='orange', marker='x')\\nplt.xlim((0,world.dimension))\\nplt.ylim((0,world.dimension))\\nplt.show()\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov5bftrep82t"
      },
      "source": [
        "#simulation.py\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class RandEvent:\n",
        "    \"\"\"A class representing a randomly generated event, as well as\n",
        "    static methods to generate a timeline of events following a\n",
        "    Poisson process.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.center = None\n",
        "        self.radius = None\n",
        "        self.time = None\n",
        "        self.duration = None\n",
        "        self.data_slice = None\n",
        "\n",
        "    def current(self, time):\n",
        "        if self.time is None or self.duration is None:\n",
        "            return False\n",
        "\n",
        "        return self.time <= time and time <= self.time + self.duration\n",
        "\n",
        "    # Returns a data item from the event's data slice\n",
        "    def observe_data(self):\n",
        "        if self.data_slice is None:\n",
        "            return None\n",
        "\n",
        "        return self.data_slice\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"center:{}, radius:{}, time:{}, duration:{}, data_slice:{}\".format(\n",
        "            self.center, self.radius, self.time, self.duration, self.data_slice)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_events_timeline(\n",
        "            timelimit,\n",
        "            rate,\n",
        "            ev_size,\n",
        "            ev_duration,\n",
        "            world,\n",
        "            fed_data_selector):\n",
        "        # Poisson process\n",
        "        events = []\n",
        "        time = 0\n",
        "\n",
        "        max_duration = 0\n",
        "\n",
        "        while time < timelimit:\n",
        "            next_time = -np.log(1.0 - np.random.uniform()) / rate\n",
        "            time += next_time\n",
        "\n",
        "            # Generate event\n",
        "            next_event = RandEvent()\n",
        "            next_event.center = world.random_point()\n",
        "            next_event.radius = np.random.normal(loc=ev_size, scale=1.0)\n",
        "            next_event.duration = np.random.normal(loc=ev_duration, scale=10.0)\n",
        "            next_event.data_slice = fed_data_selector.get_dataslice(\n",
        "                next_event.center[0], next_event.center[1])\n",
        "            next_event.time = time\n",
        "\n",
        "            if (next_event.duration > max_duration):\n",
        "                max_duration = next_event.duration\n",
        "\n",
        "            events.append((time, next_event))\n",
        "\n",
        "        return (events, max_duration)\n",
        "\n",
        "    # Returns whether \"vehicle\" matches \"event\"\n",
        "    @staticmethod\n",
        "    def match_event(vehicle, event, time):\n",
        "        return event.current(time) and World.distance(\n",
        "            vehicle.mobility.position, event.center) <= event.radius\n",
        "\n",
        "    # Returns whether \"vehicle\" matches at least one event from \"events\"\n",
        "\n",
        "    @staticmethod\n",
        "    def match_events(vehicle, events, time):\n",
        "        for e in events:\n",
        "            if RandEvent.match_event(vehicle, e, time):\n",
        "                #print (\"match! vid:{}, vpos:{}, vspeed:{} e:{}\".format(vehicle.v_id, vehicle.position, vehicle._speed, e))\n",
        "                return e\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Finds the index right before (if smallerIndex == True) or after \"value\"\n",
        "    # in \"l[start:end]\"\n",
        "    @staticmethod\n",
        "    def find_ix_sorted(l, value, start, end, smaller_index):\n",
        "\n",
        "        if start == end - 1:\n",
        "            return start if smaller_index else end\n",
        "\n",
        "        mid_point = (start + end) // 2\n",
        "        if l[mid_point] == value:\n",
        "            return mid_point\n",
        "        elif l[mid_point] < value:\n",
        "            return RandEvent.find_ix_sorted(\n",
        "                l, value, mid_point, end, smaller_index)\n",
        "        elif l[mid_point] > value:\n",
        "            return RandEvent.find_ix_sorted(\n",
        "                l, value, start, mid_point, smaller_index)\n",
        "\n",
        "    # Return the list of events happening between start_time and end_time\n",
        "    # (excluded)\n",
        "    @staticmethod\n",
        "    def select_events_between(events, start_time, end_time):\n",
        "\n",
        "        times = list((time for (time, _) in events))\n",
        "\n",
        "        begin = RandEvent.find_ix_sorted(\n",
        "            times, start_time, 0, len(times), True)\n",
        "        end = RandEvent.find_ix_sorted(times, end_time, 0, len(times), False)\n",
        "\n",
        "        return events[begin + 1:end]\n",
        "\n",
        "\n",
        "DB_DISCARD_TIME = 300\n",
        "DB_DISCARD_THRESHOLD = 500\n",
        "\n",
        "\n",
        "class Vehicle:\n",
        "    \"\"\" A class representing a vehicle. The vehicle is able to move within the bounds\n",
        "    described by a World instance, following a (steady-state) Random Waypoint mobility model.\"\"\"\n",
        "\n",
        "    def __init__(self, mobility_model, v_id):\n",
        "\n",
        "        self.v_id = v_id\n",
        "        self.mobility = mobility_model\n",
        "\n",
        "        # \"Database\"\n",
        "        self.db_last_obs = None\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return (self.v_id == other.v_id)\n",
        "\n",
        "    def drive(self, timestep):\n",
        "        self.mobility.move(timestep)\n",
        "\n",
        "    # Discard old data\n",
        "    def update_db(self, time):\n",
        "\n",
        "        if self.db_last_obs is not None:\n",
        "\n",
        "            # First condition: timeout?\n",
        "            if self.db_last_obs['time'] + DB_DISCARD_TIME < time:\n",
        "                self.db_last_obs = None\n",
        "\n",
        "            # Second condition: out of relevant area?\n",
        "            elif World.distance(self.mobility.position, self.db_last_obs['position']) > DB_DISCARD_THRESHOLD:\n",
        "                #print(\"vid {}: discard interest because of distance:{}\".format(self.v_id, World.distance(self.position, self.db_last_obs[1])))\n",
        "                self.db_last_obs = None\n",
        "\n",
        "    def rx_model_forstep(self, step_id, network_tx, time):\n",
        "\n",
        "        #print (\"vid {} rx model\".format(self.v_id))\n",
        "        if self.db_last_obs is None:\n",
        "            print(\n",
        "                \"vid {}: discarding model for step id {} received at time {}, no data\".format(\n",
        "                    self.v_id, step_id, time))\n",
        "            return\n",
        "\n",
        "        localtraining_time = np.random.normal(loc=1, scale=0.5)\n",
        "        network_tx.to_coordinator(\n",
        "            step_id,\n",
        "            self.v_id,\n",
        "            time + localtraining_time,\n",
        "            self.db_last_obs['federated_data'])\n",
        "        print(\n",
        "            \"vid {}: model for step id {} received from coordinator at time {}, training and sending back at time {}\".format(\n",
        "                self.v_id,\n",
        "                step_id,\n",
        "                time,\n",
        "                time +\n",
        "                localtraining_time))\n",
        "\n",
        "    def observe(self, time, event, network_tx):\n",
        "        #print (\"vid {}, observation at time {} and loc {}\".format(self.v_id, time, self.position))\n",
        "\n",
        "        observed_data = event.observe_data()\n",
        "        if observed_data is None:\n",
        "            return\n",
        "\n",
        "        position_cpy = np.copy(self.mobility.position)\n",
        "        self.db_last_obs = {\n",
        "            \"time\": time,\n",
        "            \"position\": position_cpy,\n",
        "            \"federated_data\": observed_data}\n",
        "        network_tx.notify_interest_to_coordinator(\n",
        "            self.v_id, position_cpy, time, time)\n",
        "\n",
        "    def observe_with_selector(self, time, fed_selector, network_tx):\n",
        "        #print (\"vid {}, observation at time {} and loc {}\".format(self.v_id, time, self.position))\n",
        "\n",
        "        if self.db_last_obs is None or self.db_last_obs['time'] + 10 <= time:\n",
        "\n",
        "            position_cpy = np.copy(self.mobility.position)\n",
        "            self.db_last_obs = {\n",
        "                \"time\": time,\n",
        "                \"position\": position_cpy,\n",
        "                \"federated_data\": fed_selector.get_dataslice(\n",
        "                    position_cpy[0],\n",
        "                    position_cpy[1])}\n",
        "            network_tx.notify_interest_to_coordinator(\n",
        "                self.v_id, position_cpy, time, time)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_vehicles_RWP(nb_vehicles, world):\n",
        "        vehicles = []\n",
        "        for i in range(nb_vehicles):\n",
        "            vehicles.append(Vehicle(RWPNode(world, i), i))\n",
        "\n",
        "        return vehicles\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_vehicles_RPGM(groups, world):\n",
        "        vehicles = []\n",
        "        mobility_nodes = generate_rpgm_mobility(groups, world)\n",
        "\n",
        "        for node in mobility_nodes:\n",
        "            vehicles.append(Vehicle(node, node.n_id))\n",
        "\n",
        "        return vehicles\n",
        "\n",
        "\n",
        "class NetworkTx:\n",
        "    \"\"\" A simple class to simulate transmission delays of messages between\n",
        "    the federated learning coordinator and the vehicles.\n",
        "    Messages to be sent are cached and passed on after a timeout.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.tx_to_vehicle = []\n",
        "        self.tx_to_coordinator = []\n",
        "        self.tx_interest_to_coordinator = []\n",
        "\n",
        "    def notify_interest_to_coordinator(\n",
        "            self, v_id, location, sense_time, transfer_time):\n",
        "\n",
        "        tx_time = np.random.normal(loc=1.0, scale=1.0)\n",
        "        self.tx_interest_to_coordinator.append(\n",
        "            (transfer_time + tx_time, v_id, location, sense_time))\n",
        "\n",
        "    # Vehicle transmits model update to coordinator (from step \"step_id\") at\n",
        "    # time \"transfer_time\"\n",
        "    def to_coordinator(self, step_id, v_id, transfer_time, federated_data):\n",
        "\n",
        "        tx_time = np.random.normal(loc=1.0, scale=1.0)\n",
        "        self.tx_to_coordinator.append(\n",
        "            (transfer_time + tx_time, v_id, step_id, federated_data))\n",
        "\n",
        "    # Coordinator transmits model for training step \"step_id\" to \"vehicle\" at\n",
        "    # time \"transfer_time\"\n",
        "    # Coordinator transmits model (step_id) to \"vehicle\"\n",
        "    def to_vehicle(self, vehicle, step_id, transfer_time):\n",
        "\n",
        "        tx_time = np.random.normal(loc=1.0, scale=1.0)\n",
        "        self.tx_to_vehicle.append((transfer_time + tx_time, vehicle, step_id))\n",
        "\n",
        "    def update(self, coordinator, time):\n",
        "        # Tx coordinator -> vehicle\n",
        "        to_keep_tovehicle = []\n",
        "        for (rx_time, vehicle, step_id) in self.tx_to_vehicle:\n",
        "            if time >= rx_time:\n",
        "                vehicle.rx_model_forstep(step_id, self, time)\n",
        "                #print (\"time {}, vid {} received model for step {}\".format(time, vehicle.v_id, step_id))\n",
        "            else:\n",
        "                to_keep_tovehicle.append((rx_time, vehicle, step_id))\n",
        "        self.tx_to_vehicle = to_keep_tovehicle\n",
        "\n",
        "        # Tx vehicle -> coordinator\n",
        "        to_keep_tocoordinator = []\n",
        "        for (rx_time, v_id, step_id, federated_data) in self.tx_to_coordinator:\n",
        "            if time >= rx_time:\n",
        "                coordinator.rx_model_update(\n",
        "                    v_id, step_id, time, federated_data)\n",
        "                #print (\"time {}, coordinator received update from vid {} for step {}\".format(time, v_id, step_id))\n",
        "            else:\n",
        "                to_keep_tocoordinator.append(\n",
        "                    (rx_time, v_id, step_id, federated_data))\n",
        "        self.tx_to_coordinator = to_keep_tocoordinator\n",
        "\n",
        "        # Tx vehicle interest -> coordinator\n",
        "        to_keep_interests = []\n",
        "        for (\n",
        "            rx_time,\n",
        "            v_id,\n",
        "            location,\n",
        "                interest_time) in self.tx_interest_to_coordinator:\n",
        "            if time >= rx_time:\n",
        "                coordinator.rx_vehicle_interest(\n",
        "                    v_id, location, interest_time, time)\n",
        "                #print (\"time {}, coordinator received model interest: vid {} time of sensing {}\".format(time, v_id, interest_time))\n",
        "            else:\n",
        "                to_keep_interests.append(\n",
        "                    (rx_time, v_id, location, interest_time))\n",
        "        self.tx_interest_to_coordinator = to_keep_interests\n",
        "\n",
        "\n",
        "INTEREST_DISCARDTIME = 300\n",
        "\n",
        "\n",
        "class Coordinator:\n",
        "    \"\"\" A class representing the FL coordinator in charge for training a model\n",
        "    based on the EMNIST dataset by the FedAvg algorithm. The coordinator initiates\n",
        "    a new training step every T=15s, in which a set of vehicles is chosen to receive\n",
        "    and locally train the current model. The coordinator then waits and receives model\n",
        "    updates back from replying vehicles, before aggregating the results.\n",
        "    Additionally, the coordinator receives training interests from vehicles,\n",
        "    that it uses for pertinent client selection in the VKN-assisted approach.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            veh_per_step,\n",
        "            fed_training,\n",
        "            fed_data_handler,\n",
        "            statistics):\n",
        "\n",
        "        self.statistics = statistics\n",
        "        self.fed_training = fed_training\n",
        "        self.fed_data_handler = fed_data_handler\n",
        "\n",
        "        self.nb_updates_total = 0\n",
        "        self.modelsent_step = []\n",
        "        self.updatesreceived_step = []\n",
        "        self.step_fed_data = []\n",
        "\n",
        "        self.step_duration = 15\n",
        "        self.vehicles_per_step = veh_per_step['veh_per_step']\n",
        "        self.vkn_adapt_vperstep = veh_per_step['vkn_adapt_vperstep']\n",
        "\n",
        "        self.step_id = -1\n",
        "        self.step_starttime = -21\n",
        "\n",
        "        self.vkn_known_interests = {}\n",
        "\n",
        "    def update(self, vehicles, time, network_tx, vkn):\n",
        "        if time > self.step_starttime + self.step_duration:\n",
        "\n",
        "            if self.step_id != -1:\n",
        "\n",
        "                # Perform federated learning, aggregation of received models at\n",
        "                # last step.\n",
        "                print(\n",
        "                    \"step {}: fed data: {}\".format(\n",
        "                        self.step_id,\n",
        "                        self.step_fed_data))\n",
        "                step_eval = self.fed_training.training_round(\n",
        "                    self.fed_data_handler.get_federated_data(self.step_fed_data))\n",
        "\n",
        "                # Statistics\n",
        "                if step_eval is not None:\n",
        "                    self.statistics.register_training_evaluation(\n",
        "                        self.step_id, step_eval['sparse_categorical_accuracy'],step_eval['loss'])\n",
        "\n",
        "                self.statistics.register_step_efficiency(self.step_id, len(\n",
        "                    self.updatesreceived_step) / self.vehicles_per_step)\n",
        "                print(\"Last step: {}/{} models OK\".format(\n",
        "                    len(self.updatesreceived_step), self.vehicles_per_step))\n",
        "\n",
        "                #print (\"Known interests count: {}\".format(len(self.vkn_known_interests.keys())))\n",
        "\n",
        "                # Remove vehicles that did not respond from the interests list\n",
        "                #print (\"self.modelsent_step:{}\".format(self.modelsent_step))\n",
        "                #print (\"self.updatesreceived_step:{}\".format(self.updatesreceived_step))\n",
        "                vehicles_noresponse = [\n",
        "                    v_id for v_id in self.modelsent_step if v_id not in self.updatesreceived_step]\n",
        "                print(\"Unresponsive vehicles: {}\".format(vehicles_noresponse))\n",
        "\n",
        "                new_interests = {}\n",
        "                for (v_id, (interest_time, position)\n",
        "                     ) in self.vkn_known_interests.items():\n",
        "                    if v_id not in self.modelsent_step or v_id in self.updatesreceived_step:\n",
        "                        new_interests[v_id] = (interest_time, position)\n",
        "                        #print(\"Keeping interest {}\".format(v_id))\n",
        "\n",
        "                self.vkn_known_interests = new_interests\n",
        "\n",
        "            self.step_fed_data = []\n",
        "            self.modelsent_step = []\n",
        "            self.updatesreceived_step = []\n",
        "\n",
        "            # Interests update\n",
        "            interests_to_keep = {}\n",
        "            for (v_id, (interest_time, position)\n",
        "                 ) in self.vkn_known_interests.items():\n",
        "                if interest_time + INTEREST_DISCARDTIME >= time:\n",
        "                    interests_to_keep[v_id] = (interest_time, position)\n",
        "            self.vkn_known_interests = interests_to_keep\n",
        "\n",
        "            if vkn and len(self.vkn_known_interests.keys()) > 0:\n",
        "                print(\"Coordinator: new step (vkn) (time={})\".format(time))\n",
        "                self.new_step_vkn(vehicles, time, network_tx)\n",
        "            else:\n",
        "                print(\"Coordinator: new step (time={})\".format(time))\n",
        "                self.new_step_random(vehicles, time, network_tx)\n",
        "\n",
        "    def new_step_vkn(self, vehicles, time, network_tx):\n",
        "\n",
        "        self.step_id += 1\n",
        "        self.step_starttime = time\n",
        "\n",
        "        selected_indexes = []\n",
        "\n",
        "        # From interests\n",
        "        #print ( \"interests:{}\".format( np.array(list(self.vkn_known_interests.keys())) ) )\n",
        "\n",
        "        sorted_interests = sorted(\n",
        "            self.vkn_known_interests.items(),\n",
        "            key=operator.itemgetter(1),\n",
        "            reverse=True)\n",
        "        sorted_vids = list((v_id for (v_id, _) in sorted_interests))\n",
        "\n",
        "        selected_indexes_interests = np.array(\n",
        "            sorted_vids[0:np.minimum(len(sorted_vids), self.vehicles_per_step)])\n",
        "        #selected_indexes_interests = np.array(list(self.vkn_known_interests.keys()))[np.random.choice(len(self.vkn_known_interests), size=np.minimum(len(self.vkn_known_interests), self.vehicles_per_step), replace=False)]\n",
        "        #print ( \"chosen interests: vids: {}\".format(selected_indexes_interests) )\n",
        "\n",
        "        for v_id in selected_indexes_interests:\n",
        "            selected_indexes.append(v_id)\n",
        "\n",
        "        nb_random_vehicles = self.vehicles_per_step - len(selected_indexes)\n",
        "\n",
        "        selected_indexes_tmp = []\n",
        "        selected_interest_pos = []\n",
        "\n",
        "        # LESS KNOWN INTERESTS THAN CLIENTS TO SELECT: SELECT ALL INTERESTS +\n",
        "        # FILL WITH RANDOM\n",
        "        if len(self.vkn_known_interests.items()) <= self.vehicles_per_step:\n",
        "            for vid in self.vkn_known_interests.keys():\n",
        "                selected_indexes.append(vid)\n",
        "\n",
        "            nb_random_vehicles = self.vehicles_per_step - len(selected_indexes)\n",
        "            if nb_random_vehicles > 0:\n",
        "                remaining_indexes = np.array([ix for ix in range(\n",
        "                    len(vehicles)) if ix not in selected_indexes_interests])\n",
        "                extra_selected_indexes = remaining_indexes[np.random.choice(\n",
        "                    len(remaining_indexes), size=nb_random_vehicles, replace=False)]\n",
        "\n",
        "                for v_id in extra_selected_indexes:\n",
        "                    selected_indexes.append(v_id)\n",
        "\n",
        "        else:  # Enough interests: CLUSTERING OF INTERESTS\n",
        "\n",
        "            interests_vid = []\n",
        "            interests_times = []\n",
        "            interests_locations = []\n",
        "            for (vid, (interest_time, loc)\n",
        "                 ) in self.vkn_known_interests.items():\n",
        "                interests_vid.append(vid)\n",
        "                interests_times.append(interest_time)\n",
        "                interests_locations.append(loc)\n",
        "\n",
        "            interests_vid = np.array(interests_vid)\n",
        "            interests_times = np.array(interests_times)\n",
        "            interests_locations = np.array(interests_locations)\n",
        "\n",
        "            kmeans = KMeans(\n",
        "                n_clusters=self.vehicles_per_step,\n",
        "                tol=10,\n",
        "                max_iter=30)\n",
        "            kmeans.fit(interests_locations)\n",
        "\n",
        "            # Taking the most recent vehicles by cluster.\n",
        "            for cluster_id in range(self.vehicles_per_step):\n",
        "                print(\"cluster {}\".format(cluster_id))\n",
        "\n",
        "                cluster_point_ids = []\n",
        "                for j in range(len(interests_vid)):\n",
        "                    if kmeans.labels_[j] == cluster_id:\n",
        "                        cluster_point_ids.append(j)\n",
        "\n",
        "                cluster_point_ids = np.array(cluster_point_ids)\n",
        "                newest_interest = -1\n",
        "                newest_time = -1\n",
        "                newest_location = -1\n",
        "                for point_id in cluster_point_ids:\n",
        "\n",
        "                    if newest_interest == - \\\n",
        "                            1 or interests_times[point_id] > newest_time:\n",
        "                        newest_interest = interests_vid[point_id]\n",
        "                        newest_time = interests_times[point_id]\n",
        "                        newest_location = interests_locations[point_id]\n",
        "\n",
        "                '''locs = interests_locations[cluster_point_ids]\n",
        "                print (locs)\n",
        "\n",
        "                plt.scatter(locs[:, 0], locs[:, 1], color='orange')\n",
        "                plt.scatter(kmeans.cluster_centers_[cluster_id,0], kmeans.cluster_centers_[cluster_id,1], color='green')\n",
        "                plt.xlim((0,1000))\n",
        "                plt.ylim((0,1000))\n",
        "                plt.show()'''\n",
        "\n",
        "                selected_interest_pos.append(newest_location)\n",
        "                selected_indexes_tmp.append(newest_interest)\n",
        "\n",
        "            # Final step: Reduce amount of clients selected if enabled\n",
        "            remove_indexes = []\n",
        "            if self.vkn_adapt_vperstep:\n",
        "                for (i, pos1) in enumerate(selected_interest_pos):\n",
        "\n",
        "                    finished = False\n",
        "                    j = i + 1\n",
        "                    while j < len(selected_interest_pos) and not finished:\n",
        "                        pos2 = selected_interest_pos[j]\n",
        "                        if World.distance(pos1, pos2) <= 50:\n",
        "                            remove_indexes.append(i)\n",
        "                            finished = True\n",
        "                        j += 1\n",
        "\n",
        "            selected_indexes = []\n",
        "            for (ix, value) in enumerate(selected_indexes_tmp):\n",
        "                if ix not in remove_indexes:\n",
        "                    selected_indexes.append(value)\n",
        "\n",
        "            print(\"NB CLIENTS SELECTED: {}\".format(len(selected_indexes)))\n",
        "\n",
        "            '''\n",
        "            debug = np.array(vehicles)[np.array(selected_indexes)]\n",
        "            selected_vehicles_pos = []\n",
        "            for v in debug:\n",
        "                selected_vehicles_pos.append(v.mobility.position)\n",
        "            selected_vehicles_pos = np.array(selected_vehicles_pos)\n",
        "\n",
        "            all_vehicles_pos = []\n",
        "            for v in vehicles:\n",
        "                all_vehicles_pos.append(v.mobility.position)\n",
        "            all_vehicles_pos = np.array(all_vehicles_pos)\n",
        "\n",
        "            random_vehs = np.random.choice(vehicles, 10)\n",
        "            random_vehicles_pos = []\n",
        "            for v in random_vehs:\n",
        "                random_vehicles_pos.append(v.mobility.position)\n",
        "            random_vehicles_pos = np.array(random_vehicles_pos)\n",
        "\n",
        "            plt.title(\"Clustering of training samples' location of sensing by the coordinator\")\n",
        "            plt.scatter(all_vehicles_pos[:, 0], all_vehicles_pos[:, 1], color='gray', alpha=0.3) # All vehicles\n",
        "            plt.scatter(random_vehicles_pos[:,0], random_vehicles_pos[:,1], color='orange', marker='x') # Random selected v\n",
        "            plt.scatter(selected_vehicles_pos[:,0], selected_vehicles_pos[:,1], color='blue', marker='x') # VKN selected v\n",
        "            plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], color='purple', marker='$c$') # Clusters\n",
        "            plt.xlim((0,1000))\n",
        "            plt.ylim((0,1000))\n",
        "            plt.show()'''\n",
        "\n",
        "        #print (\"final selected indexes: selected vids:{}\".format(selected_indexes))\n",
        "\n",
        "        self.statistics.register_step_nbselect(\n",
        "            self.step_id, len(selected_indexes))\n",
        "        selected_vehicles = np.array(vehicles)[np.array(selected_indexes)]\n",
        "\n",
        "        print(\"step_id:{}, sent to vehicles:\".format(self.step_id))\n",
        "\n",
        "        for v in selected_vehicles:\n",
        "            network_tx.to_vehicle(v, self.step_id, time)\n",
        "            print(v.v_id)\n",
        "\n",
        "        self.modelsent_step = selected_indexes\n",
        "\n",
        "    def new_step_random(self, vehicles, time, network_tx):\n",
        "\n",
        "        self.step_id += 1\n",
        "        self.step_starttime = time\n",
        "\n",
        "        selected_indexes = np.random.choice(\n",
        "            len(vehicles), size=self.vehicles_per_step, replace=False)\n",
        "        selected_vehicles = np.array(vehicles)[selected_indexes]\n",
        "        print(\"step_id:{}, sent to vehicles:\".format(self.step_id))\n",
        "\n",
        "        for v in selected_vehicles:\n",
        "            network_tx.to_vehicle(v, self.step_id, time)\n",
        "            print(v.v_id)\n",
        "\n",
        "        self.modelsent_step = selected_indexes\n",
        "\n",
        "    # Reception of a model update from vehicle\n",
        "    def rx_model_update(self, v_id, step_id, time, federated_data):\n",
        "        if step_id != self.step_id:  # An old model, discard\n",
        "            print(\n",
        "                \"time {} vid {} stepid {} != current step id {}: Model discarded\".format(\n",
        "                    time, v_id, step_id, self.step_id))\n",
        "            return\n",
        "\n",
        "        print(\n",
        "            \"Model update received at time {}, vid:{} / stepid:{} ; federated data id: {}\".format(\n",
        "                time,\n",
        "                v_id,\n",
        "                step_id,\n",
        "                federated_data))\n",
        "        self.step_fed_data.append(federated_data)\n",
        "        self.nb_updates_total += 1\n",
        "        self.updatesreceived_step.append(v_id)\n",
        "\n",
        "        self.statistics.register_new_step(time, v_id)\n",
        "\n",
        "    def rx_vehicle_interest(self, v_id, location, interest_time, rx_time):\n",
        "\n",
        "        #print (\"time {}: Coordinator received interest (vid:{}, location: {}, interest time:{})\".format(rx_time, v_id, location, interest_time))\n",
        "        self.vkn_known_interests[v_id] = (\n",
        "            interest_time, [location[0], location[1]])\n",
        "\n",
        "\n",
        "class Statistics:\n",
        "    \"\"\" A simple class to keep track of the progress of a simulation.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.steps_per_time = {}\n",
        "        self.step_efficiency = {}\n",
        "        self.step_nbselect = {}\n",
        "\n",
        "        self.training_accuracy = {}\n",
        "        self.training_loss = {}\n",
        "\n",
        "        self.participating_vids = []\n",
        "        self.newvids_per_time = {}\n",
        "\n",
        "    def register_new_step(self, time, v_id):\n",
        "\n",
        "        if time not in self.newvids_per_time:\n",
        "            self.newvids_per_time[time] = 0\n",
        "\n",
        "        if v_id not in self.participating_vids:\n",
        "            self.participating_vids.append(v_id)\n",
        "            self.newvids_per_time[time] += 1\n",
        "\n",
        "        if time not in self.steps_per_time:\n",
        "            self.steps_per_time[time] = 0\n",
        "\n",
        "        self.steps_per_time[time] += 1\n",
        "\n",
        "    def register_step_efficiency(self, step_id, rate):\n",
        "        self.step_efficiency[step_id] = rate\n",
        "\n",
        "    def register_step_nbselect(self, step_id, nbselect):\n",
        "        self.step_nbselect[step_id] = nbselect\n",
        "\n",
        "    def register_training_evaluation(self, step_id, accuracy, loss):\n",
        "        self.training_accuracy[step_id] = accuracy\n",
        "        self.training_loss[step_id] = loss\n",
        "\n",
        "    def get_total_steps_per_time(self):\n",
        "\n",
        "        # Total steps per time. [0] (time) and [1] (aggregated steps)\n",
        "        times = []\n",
        "        total_steps = []\n",
        "\n",
        "        current_total = 0\n",
        "        for time in sorted(self.steps_per_time.keys()):\n",
        "            current_total += self.steps_per_time[time]\n",
        "\n",
        "            total_steps.append(current_total)\n",
        "            times.append(time)\n",
        "\n",
        "        return (times, total_steps)\n",
        "\n",
        "    def get_cumulated_newvids_per_time(self):\n",
        "\n",
        "        # Total steps per time. [0] (time) and [1] (aggregated steps)\n",
        "        times = []\n",
        "        total_vids = []\n",
        "\n",
        "        current_total = 0\n",
        "        for time in sorted(self.newvids_per_time.keys()):\n",
        "            current_total += self.newvids_per_time[time]\n",
        "\n",
        "            total_vids.append(current_total)\n",
        "            times.append(time)\n",
        "\n",
        "        return (times, total_vids)\n",
        "\n",
        "    def plot_total_steps_per_time(self):\n",
        "\n",
        "        (times, total_steps) = self.get_total_steps_per_time()\n",
        "\n",
        "        plt.plot(times, total_steps)\n",
        "        plt.show()\n",
        "\n",
        "    # Smoothed dict {time:value}\n",
        "\n",
        "    def get_smoothed_list(self, data, smooth):\n",
        "        x = []\n",
        "        y = []\n",
        "\n",
        "        times = list(sorted(data.keys()))\n",
        "\n",
        "        smooth_value = 0\n",
        "        smooth_index = 0\n",
        "        for timeval in times:\n",
        "\n",
        "            smooth_value += data[timeval]\n",
        "            smooth_index += 1\n",
        "\n",
        "            if smooth_index == smooth:\n",
        "                x.append(timeval)\n",
        "                y.append(smooth_value / smooth)\n",
        "                smooth_index = 0\n",
        "                smooth_value = 0\n",
        "\n",
        "        return (x, y)\n",
        "\n",
        "    def get_step_efficiency(self, smooth):\n",
        "        return self.get_smoothed_list(self.step_efficiency, smooth)\n",
        "\n",
        "    def get_step_nbselect(self, smooth):\n",
        "        return self.get_smoothed_list(self.step_nbselect, smooth)\n",
        "\n",
        "    def get_training_accuracy(self, smooth):\n",
        "        return self.get_smoothed_list(self.training_accuracy, smooth)\n",
        "\n",
        "    def get_training_loss(self, smooth):\n",
        "        return self.get_smoothed_list(self.training_loss, smooth)\n",
        "\n",
        "    def plot_step_efficiency(self, smooth):\n",
        "        (x, y) = self.get_step_efficiency(smooth)\n",
        "        plt.plot(x, y)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_training_accuracy(self, smooth):\n",
        "        (x, y) = self.get_training_accuracy(smooth)\n",
        "        plt.title(\"Training accuracy\")\n",
        "        plt.plot(x, y)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_training_loss(self, smooth):\n",
        "        (x, y) = self.get_training_loss(smooth)\n",
        "        plt.title(\"Training loss\")\n",
        "        plt.plot(x, y)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "class Simulation:\n",
        "    \"\"\" A class to initialize and run a simulation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            size,\n",
        "            nb_vehicles,\n",
        "            mobility,\n",
        "            event_conf,\n",
        "            timelimit,\n",
        "            timestep,\n",
        "            veh_per_step,\n",
        "            vkn,\n",
        "            seed):\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        FLTraining.set_seed(seed)\n",
        "\n",
        "        self.time = 0\n",
        "        self.timelimit = timelimit\n",
        "        self.timestep = timestep\n",
        "        self.vkn = vkn\n",
        "        self.veh_per_step = veh_per_step\n",
        "\n",
        "        self.world = World(size)\n",
        "\n",
        "        self.vehicles = []\n",
        "        if mobility['model'] == \"RPGM\" and 'groups' in mobility:\n",
        "            self.vehicles = Vehicle.generate_vehicles_RPGM(\n",
        "                mobility['groups'], self.world)\n",
        "            print(\"Using RPGM model: v:{} groups:{}\".format(\n",
        "                sum(mobility['groups']), mobility['groups']))\n",
        "\n",
        "        else:  # Random Waypoint\n",
        "            self.vehicles = Vehicle.generate_vehicles_RWP(\n",
        "                nb_vehicles, self.world)\n",
        "\n",
        "        self.fed_data = FLData()\n",
        "        self.fed_training = FLTraining(\n",
        "            self.fed_data.get_federated_test_data(\n",
        "                self.fed_data.get_all_test_samples_id()[\n",
        "                    0:20]))\n",
        "\n",
        "        # Events generation\n",
        "        use_events = event_conf['use_events']\n",
        "        event_data_distribution = event_conf['data_distribution']\n",
        "        cut = 0\n",
        "        if 'cut' in event_conf:\n",
        "            cut = event_conf['cut']\n",
        "\n",
        "        self.fed_data_selector = FLDataSelector(\n",
        "            self.world.dimension, event_data_distribution, self.fed_data, cut)\n",
        "        (self.events, self.events_max_duration) = (None, None)\n",
        "\n",
        "        if use_events:\n",
        "            event_rate = event_conf['rate']\n",
        "            event_size = event_conf['size']\n",
        "            event_duration = event_conf['duration']\n",
        "            (self.events, self.events_max_duration) = RandEvent.generate_events_timeline(\n",
        "                self.timelimit, event_rate, event_size, event_duration, self.world, self.fed_data_selector)\n",
        "\n",
        "        self.statistics = Statistics()\n",
        "        self.coordinator = Coordinator(\n",
        "            self.veh_per_step,\n",
        "            self.fed_training,\n",
        "            self.fed_data,\n",
        "            self.statistics)\n",
        "        self.network_tx = NetworkTx()\n",
        "\n",
        "    def step(self):\n",
        "\n",
        "        self.time += self.timestep\n",
        "\n",
        "        # Currently happening events\n",
        "        current_events = None\n",
        "        if self.events is not None:\n",
        "            current_events = RandEvent.select_events_between(\n",
        "                self.events, self.time - self.events_max_duration - 0.01, self.time + 0.01)\n",
        "\n",
        "        # Drive vehicles\n",
        "        for v in self.vehicles:\n",
        "            v.drive(self.timestep)\n",
        "            v.update_db(self.time)\n",
        "\n",
        "            # Match events\n",
        "            if current_events is None:\n",
        "                v.observe_with_selector(\n",
        "                    self.time, self.fed_data_selector, self.network_tx)\n",
        "            else:\n",
        "                ev = RandEvent.match_events(\n",
        "                    v, (ev for (_, ev) in current_events), self.time)\n",
        "                if ev is not None:\n",
        "                    v.observe(self.time, ev, self.network_tx)\n",
        "\n",
        "        #print (\"time: {}\".format(self.time))\n",
        "        self.coordinator.update(\n",
        "            self.vehicles,\n",
        "            self.time,\n",
        "            self.network_tx,\n",
        "            vkn=self.vkn)\n",
        "        self.network_tx.update(self.coordinator, self.time)\n",
        "\n",
        "        return self.time\n",
        "\n",
        "    def done(self):\n",
        "        return self.time >= self.timelimit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "sUBKQsJAqV9f",
        "outputId": "8a468246-fbc3-473e-9fde-0ac7a3d5b85a"
      },
      "source": [
        "#rud_fed.py\n",
        "import os\n",
        "import sys\n",
        "#import argparse\n",
        "import easydict      #jupyter에서서 argparse라이브러리리 사용 시 에러발생\n",
        "import pickle\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "class NoStdStreams(object):\n",
        "    def __init__(self, stdout=None, stderr=None):\n",
        "        self.devnull = open(os.devnull, 'w')\n",
        "        self._stdout = stdout or self.devnull or sys.stdout\n",
        "        self._stderr = stderr or self.devnull or sys.stderr\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr\n",
        "        self.old_stdout.flush()\n",
        "        self.old_stderr.flush()\n",
        "        sys.stdout, sys.stderr = self._stdout, self._stderr\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self._stdout.flush()\n",
        "        self._stderr.flush()\n",
        "        sys.stdout = self.old_stdout\n",
        "        sys.stderr = self.old_stderr\n",
        "        self.devnull.close()\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser(description='Run FL/VKN simulations.')\n",
        "# parser.add_argument(\n",
        "#     '--mobility',\n",
        "#     default=\"RWP\",\n",
        "#     help='=\"RWP\" or \"RPGM\". Generate simulation results using Random Waypoint or RPGM mobility.')\n",
        "# parser.add_argument(\n",
        "#     '--seed_begin',\n",
        "#     type=int,\n",
        "#     default=0,\n",
        "#     help='Simulations will be generated for random seeds [seed_begin;seed_end[.')\n",
        "# parser.add_argument(\n",
        "#     '--seed_end',\n",
        "#     type=int,\n",
        "#     default=1,\n",
        "#     help='Simulations will be generated for random seeds [seed_begin;seed_end[.')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "    \"mobility\" : \"RWP\",\n",
        "    \"seed_begin\" : 0,\n",
        "    \"seed_end\" : 1\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Simulation's input parameters\n",
        "events_conf = {}\n",
        "mob = {}\n",
        "training = {}\n",
        "mobility_name = \"\"\n",
        "vkn_adapt_vperstep = False\n",
        "\n",
        "vps = 10  # Number of training vehicles per step\n",
        "\n",
        "if args.mobility == \"RWP\":\n",
        "\n",
        "    events_conf = {\n",
        "        \"use_events\": True,\n",
        "        \"rate\": 1.5 / 60.0,\n",
        "        \"size\": 50.0,\n",
        "        \"duration\": 60.0,\n",
        "        \"data_distribution\": \"random\"}\n",
        "    mob = {\"model\": 'RWP'}\n",
        "    training = {\"veh_per_step\": 10, \"vkn_adapt_vperstep\": False}\n",
        "\n",
        "    mobility_name = \"rwp\"\n",
        "    vkn_adapt_vperstep = False\n",
        "\n",
        "else:\n",
        "    vkn_adapt_vperstep = True\n",
        "    events_conf = {\n",
        "        \"use_events\": False,\n",
        "        \"data_distribution\": \"localized\",\n",
        "        \"cut\": 10}\n",
        "    mob = {\"model\": 'RPGM', \"groups\": [650, 20, 20, 10, 10, 10, 10, 10, 5, 5]}\n",
        "    training = {\"veh_per_step\": vps, \"vkn_adapt_vperstep\": True}\n",
        "\n",
        "    mobility_name = \"rpgm\"\n",
        "    vkn_adapt_vperstep = True\n",
        "\n",
        "\n",
        "stats_vkn = {}\n",
        "stats_tradi = {}\n",
        "\n",
        "stats_vkn[vps] = []\n",
        "stats_tradi[vps] = []\n",
        "\n",
        "\n",
        "for i in range(args.seed_begin, args.seed_end):\n",
        "\n",
        "    print(\"New simulations with seed: {}\".format(i))\n",
        "\n",
        "    simvkn = Simulation(\n",
        "        size=1000,\n",
        "        nb_vehicles=750,\n",
        "        mobility=mob,\n",
        "        event_conf=events_conf,\n",
        "        timelimit=3600,\n",
        "        timestep=1,\n",
        "        veh_per_step=training,\n",
        "        vkn=True,\n",
        "        seed=i)\n",
        "    simtradi = Simulation(\n",
        "        size=1000,\n",
        "        nb_vehicles=750,\n",
        "        mobility=mob,\n",
        "        event_conf=events_conf,\n",
        "        timelimit=3600,\n",
        "        timestep=1,\n",
        "        veh_per_step=training,\n",
        "        vkn=False,\n",
        "        seed=i)\n",
        "\n",
        "    print(\"\\tVKN simulation...\")\n",
        "    while not simvkn.done():\n",
        "        with NoStdStreams():\n",
        "            time = simvkn.step()\n",
        "        print(\"time: {}/3600s\".format(time), end=\"\\r\")\n",
        "    stats_vkn[vps].append(simvkn.statistics)\n",
        "\n",
        "    print(\"\\tvkn ok\")\n",
        "    print(\"\\tTraditional simulation...\")\n",
        "    while not simtradi.done():\n",
        "        with NoStdStreams():\n",
        "            time = simtradi.step()\n",
        "        print(\"time: {}/3600s\".format(time), end=\"\\r\")\n",
        "    stats_tradi[vps].append(simtradi.statistics)\n",
        "\n",
        "\n",
        "prefix = \"\"\n",
        "if vkn_adapt_vperstep:\n",
        "    prefix = \"optimized_\"\n",
        "\n",
        "stats = {'vkn': stats_vkn, 'tradi': stats_tradi}\n",
        "filename = \"{}{}_dump{}_{}_{}\".format(\n",
        "    prefix,\n",
        "    mobility_name,\n",
        "    vps,\n",
        "    args.seed_begin,\n",
        "    args.seed_end)\n",
        "with open(filename, 'wb') as filehandler:\n",
        "    pickle.dump(stats, filehandler)\n",
        "\n",
        "print('done.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New simulations with seed: 0\n",
            "\tVKN simulation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-aacd3970bd28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msimvkn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mNoStdStreams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimvkn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time: {}/3600s\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mstats_vkn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimvkn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4e9edc0051c8>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_tx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m             vkn=self.vkn)\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_tx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-4e9edc0051c8>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, vehicles, time, network_tx, vkn)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstep_eval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     self.statistics.register_training_evaluation(\n\u001b[0;32m--> 354\u001b[0;31m                         self.step_id, step_eval['sparse_categorical_accuracy'],step_eval['loss'])\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m                 self.statistics.register_step_efficiency(self.step_id, len(\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sparse_categorical_accuracy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7WcRhIWqit_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}